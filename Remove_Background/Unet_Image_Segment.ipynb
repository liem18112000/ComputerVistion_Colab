{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I820H0Z6t9ad"
   },
   "source": [
    "# Image Segmentation with U-Net Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mc9fZdYo1jZg"
   },
   "source": [
    "## I. Download & Manipulate Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IphZHMDoubJl"
   },
   "source": [
    "### 1. Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SKjBQKph1DEd"
   },
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/tensorflow/examples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRsdeeJt1by2"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\r\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\r\n",
    "\r\n",
    "import tensorflow_datasets as tfds\r\n",
    "\r\n",
    "from IPython.display import clear_output\r\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tl4IR8ma1mtU"
   },
   "outputs": [],
   "source": [
    "dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t25odWr92Io2"
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (224, 224)\r\n",
    "INPUT_SHAPE = (224, 224, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62N8S2f7ulEV"
   },
   "source": [
    "### 2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5q875RB2HI-"
   },
   "outputs": [],
   "source": [
    "@tf.function\r\n",
    "def load_image_train(datapoint):\r\n",
    "    input_image = tf.image.resize(datapoint['image'], IMAGE_SIZE)\r\n",
    "    input_mask = tf.image.resize(datapoint['segmentation_mask'], IMAGE_SIZE)\r\n",
    "\r\n",
    "    if tf.random.uniform(()) > 0.5:\r\n",
    "        input_image = tf.image.flip_left_right(input_image)\r\n",
    "        input_mask = tf.image.flip_left_right(input_mask)\r\n",
    "\r\n",
    "    input_image, input_mask = normalize(input_image, input_mask)\r\n",
    "\r\n",
    "    return input_image, input_mask\r\n",
    "\r\n",
    "def load_image_test(datapoint):\r\n",
    "    input_image = tf.image.resize(datapoint['image'], IMAGE_SIZE)\r\n",
    "    input_mask = tf.image.resize(datapoint['segmentation_mask'], IMAGE_SIZE)\r\n",
    "\r\n",
    "    input_image, input_mask = normalize(input_image, input_mask)\r\n",
    "\r\n",
    "    return input_image, input_mask\r\n",
    "\r\n",
    "def normalize(input_image, input_mask):\r\n",
    "    input_image = tf.cast(input_image, tf.float32) / 255.0\r\n",
    "    input_mask -= 1\r\n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hJK_Bw2ur8X"
   },
   "source": [
    "### 3. Separate the train-test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsZwL6pa2_m6"
   },
   "outputs": [],
   "source": [
    "TRAIN_LENGTH = info.splits['train'].num_examples\r\n",
    "BATCH_SIZE = 128\r\n",
    "BUFFER_SIZE = 1000\r\n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DuwFL4FN3RZc"
   },
   "outputs": [],
   "source": [
    "train = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.AUTOTUNE)\r\n",
    "test = dataset['test'].map(load_image_test)\r\n",
    "\r\n",
    "train_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\r\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\r\n",
    "test_dataset = test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bt8ZAvnlvd3i"
   },
   "source": [
    "## II. Dataset Ultility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxF-DXIHvk5H"
   },
   "source": [
    "### 1. Display the overall images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EmmJ7zee3Y3z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "def display(display_list, visible_mask = np.array([True, True, True, True])):\r\n",
    "    plt.figure(figsize=(20, 20))\r\n",
    "    title = [\"Input Image\", \"True Mask\", \"Predicted Mask\", \"Remove Background\"]\r\n",
    "    title = tf.boolean_mask(title, visible_mask).numpy().astype('str')\r\n",
    "    for i in range(len(display_list)):\r\n",
    "        plt.subplot(1, len(display_list), i+1)\r\n",
    "        plt.title(title[i])\r\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\r\n",
    "        plt.axis('off')\r\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jTP0p9hvq0C"
   },
   "source": [
    "### Mask convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBlqvkys6s8l"
   },
   "outputs": [],
   "source": [
    "def convert_mask(t, target = 1.0):\r\n",
    "    return tf.math.abs( \r\n",
    "        tf.math.subtract(\r\n",
    "            t, \r\n",
    "            tf.convert_to_tensor(target, dtype=tf.float32) \r\n",
    "        ) \r\n",
    "    )\r\n",
    "\r\n",
    "def remove_background(image, mask):\r\n",
    "    msk = convert_mask(mask)\r\n",
    "    return tf.math.multiply(image, msk)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osalRNebv4Wp"
   },
   "source": [
    "### 3. Some example from train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q9w4zdtd3qjr"
   },
   "outputs": [],
   "source": [
    "for image, mask in train.take(5):\r\n",
    "    sample_image, sample_mask = image, mask\r\n",
    "    result = remove_background(image, mask)\r\n",
    "    display([sample_image, sample_mask, result], [True, True, False, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhn9vwWEYJ5O"
   },
   "source": [
    "## III. Construct model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQWHpnaowFqr"
   },
   "source": [
    "### 1. U-Net model use MobileNetV2 Encode-Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nU-Y5G7YN9_"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2 as Pretrained_Model\r\n",
    "from tensorflow.keras.layers import *\r\n",
    "\r\n",
    "def unet_model_1(output_channels):\r\n",
    "\r\n",
    "    base_model = Pretrained_Model(\r\n",
    "        input_shape=INPUT_SHAPE, \r\n",
    "        include_top=False\r\n",
    "    )\r\n",
    "\r\n",
    "    # Use the activations of these layers\r\n",
    "    layer_names = [\r\n",
    "        'block_1_expand_relu',   # 64x64\r\n",
    "        'block_3_expand_relu',   # 32x32\r\n",
    "        'block_6_expand_relu',   # 16x16\r\n",
    "        'block_13_expand_relu',  # 8x8\r\n",
    "        'block_16_expand_relu',  # 4x4\r\n",
    "    ]\r\n",
    "\r\n",
    "    layers = [base_model.get_layer(name).output for name in layer_names]\r\n",
    "\r\n",
    "    # Create the feature extraction model\r\n",
    "    down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\r\n",
    "\r\n",
    "    down_stack.trainable = False\r\n",
    "\r\n",
    "    up_stack = [\r\n",
    "        pix2pix.upsample(1024, 3),  # 4x4 -> 8x8\r\n",
    "        pix2pix.upsample(512, 3),  # 8x8 -> 16x16\r\n",
    "        pix2pix.upsample(256, 3),  # 16x16 -> 32x32\r\n",
    "        pix2pix.upsample(128, 3),   # 32x32 -> 64x64\r\n",
    "    ]\r\n",
    "\r\n",
    "    inputs = Input(shape=INPUT_SHAPE)\r\n",
    "    x = inputs\r\n",
    "\r\n",
    "    # Downsampling through the model\r\n",
    "    skips = down_stack(x)\r\n",
    "    x = skips[-1]\r\n",
    "    skips = reversed(skips[:-1])\r\n",
    "\r\n",
    "    # Upsampling and establishing the skip connections\r\n",
    "    for up, skip in zip(up_stack, skips):\r\n",
    "        x = up(x)\r\n",
    "        concat = Concatenate()\r\n",
    "        x = concat([x, skip])\r\n",
    "\r\n",
    "    # This is the last layer of the model\r\n",
    "    last = Conv2DTranspose(\r\n",
    "        output_channels, 3, strides=2,\r\n",
    "        padding='same'\r\n",
    "    )  #64x64 -> 128x128\r\n",
    "\r\n",
    "    x = last(x)\r\n",
    "\r\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgHrOfbgwQex"
   },
   "source": [
    "### 2. U-Net model use MobileNetV3Large Encode-Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUnNIbV-f0Mv"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV3Large as Pretrained_Model_2\r\n",
    "from tensorflow.keras.layers import *\r\n",
    "\r\n",
    "def unet_model_2(output_channels):\r\n",
    "\r\n",
    "    base_model = Pretrained_Model_2(\r\n",
    "        input_shape=INPUT_SHAPE, \r\n",
    "        include_top=False\r\n",
    "    )\r\n",
    "\r\n",
    "    for index, layer in enumerate(base_model.layers):\r\n",
    "        layer._name = 'mylayer_' + str(index)\r\n",
    "\r\n",
    "    # Use the activations of these layers\r\n",
    "    layer_names = [\r\n",
    "        'mylayer_16',   # 64x64\r\n",
    "        'mylayer_28',   # 32x32\r\n",
    "        'mylayer_89',   # 16x16\r\n",
    "        'mylayer_178',  # 8x8\r\n",
    "        'mylayer_273'   # 4x4\r\n",
    "    ]\r\n",
    "\r\n",
    "    layers = [base_model.get_layer(name).output for name in layer_names]\r\n",
    "\r\n",
    "    # Create the feature extraction model\r\n",
    "    down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\r\n",
    "\r\n",
    "    down_stack.trainable = False\r\n",
    "\r\n",
    "    up_stack = [\r\n",
    "        pix2pix.upsample(1024, 3),  # 4x4 -> 8x8\r\n",
    "        pix2pix.upsample(512, 3),  # 8x8 -> 16x16\r\n",
    "        pix2pix.upsample(256, 3),  # 16x16 -> 32x32\r\n",
    "        pix2pix.upsample(128, 3),   # 32x32 -> 64x64\r\n",
    "    ]\r\n",
    "\r\n",
    "    inputs = Input(shape=INPUT_SHAPE)\r\n",
    "    x = inputs\r\n",
    "\r\n",
    "    # Downsampling through the model\r\n",
    "    skips = down_stack(x)\r\n",
    "    x = skips[-1]\r\n",
    "    skips = reversed(skips[:-1])\r\n",
    "\r\n",
    "    # Upsampling and establishing the skip connections\r\n",
    "    for up, skip in zip(up_stack, skips):\r\n",
    "        x = up(x)\r\n",
    "        concat = Concatenate()\r\n",
    "        x = concat([x, skip])\r\n",
    "\r\n",
    "    # This is the last layer of the model\r\n",
    "    last = Conv2DTranspose(\r\n",
    "        output_channels, 3, strides=2,\r\n",
    "        padding='same'\r\n",
    "    )  #64x64 -> 128x128\r\n",
    "\r\n",
    "    x = last(x)\r\n",
    "\r\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgWzszsC640B"
   },
   "source": [
    "### 3. Self-made Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PKUAttp687m"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\r\n",
    "\r\n",
    "def bottleneck(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1, numberOfLayer = 2):\r\n",
    "    input_layer = x\r\n",
    "    for i in range(numberOfLayer):\r\n",
    "        input_layer = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(input_layer)\r\n",
    "    return input_layer\r\n",
    "\r\n",
    "def down_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1, numberOfLayer = 2):\r\n",
    "    c = bottleneck(x, filters, kernel_size, padding, strides, numberOfLayer)\r\n",
    "    p = MaxPool2D((2, 2), (2, 2))(c)\r\n",
    "    return c, p\r\n",
    "\r\n",
    "def up_block(x, skip, filters, kernel_size=(3, 3), padding=\"same\", strides=1, numberOfLayer = 2):\r\n",
    "    us = UpSampling2D((2, 2))(x)\r\n",
    "    concat = Concatenate()([us, skip])\r\n",
    "    c = bottleneck(concat, filters, kernel_size, padding, strides, numberOfLayer)\r\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ZEfiq6W64Zo"
   },
   "outputs": [],
   "source": [
    "def unet_model_3(channels):\r\n",
    "    f = [32, 64, 128, 256, 512, 1024]\r\n",
    "    numOfLayer = 4\r\n",
    "    inputs = Input(INPUT_SHAPE)\r\n",
    "    \r\n",
    "    conv = []\r\n",
    "    pool = inputs\r\n",
    "    for i in range(len(f) - 1):\r\n",
    "        c, pool = down_block(pool, f[1], numberOfLayer=numOfLayer)\r\n",
    "        conv.append(c)\r\n",
    "\r\n",
    "    # c1, p1 = down_block(p0, f[0]) #128 -> 64\r\n",
    "    # c2, p2 = down_block(p1, f[1]) #64 -> 32\r\n",
    "    # c3, p3 = down_block(p2, f[2]) #32 -> 16\r\n",
    "    # c4, p4 = down_block(p3, f[3]) #16->8\r\n",
    "    \r\n",
    "    bn = bottleneck(pool, f[-1])\r\n",
    "    up = bn\r\n",
    "    \r\n",
    "    for i in range(len(f) - 2, 0, -1):\r\n",
    "        up = up_block(up, conv.pop(), f[i], numberOfLayer=numOfLayer)\r\n",
    "\r\n",
    "    # u1 = up_block(bn, c4, f[3]) #8 -> 16\r\n",
    "    # u2 = up_block(u1, c3, f[2]) #16 -> 32\r\n",
    "    # u3 = up_block(u2, c2, f[1]) #32 -> 64\r\n",
    "    # u4 = up_block(u3, c1, f[0]) #64 -> 128\r\n",
    "\r\n",
    "    # outputs = Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(up)\r\n",
    "    outputs = Conv2DTranspose(channels, 3, strides=2, padding='same', activation='softmax')(up)\r\n",
    "\r\n",
    "    model = tf.keras.models.Model(inputs, outputs)\r\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXtSaQ4JwbDx"
   },
   "source": [
    "## IV. Let's get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQo7ha-t9Pet"
   },
   "source": [
    "### 0. Model Ultility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiz59n269URJ"
   },
   "outputs": [],
   "source": [
    "class ModelManager(object):\r\n",
    "\r\n",
    "    def __init__(self):\r\n",
    "        self._ModelRegistry_ = {}\r\n",
    "\r\n",
    "    def is_key_exist(self, key):\r\n",
    "        return key in self._ModelRegistry_.keys()\r\n",
    "\r\n",
    "    def is_model_exist(self, model):\r\n",
    "        return model in self._ModelRegistry_.values()\r\n",
    "\r\n",
    "    def register_model(self, key, model):\r\n",
    "        if self.is_key_exist(key):\r\n",
    "            if self.__ModelRegistry_[key] == model:\r\n",
    "                return model\r\n",
    "            print(\"Key exist in registry and registered for another model!\")\r\n",
    "            return model\r\n",
    "\r\n",
    "        if self.is_model_exist(model):\r\n",
    "            if self._ModelRegistry_.index(model) == key:\r\n",
    "                return model\r\n",
    "            print('Model is already registred with index : ' + str(self._ModelRegistry_.index(model)))\r\n",
    "            return model\r\n",
    "\r\n",
    "        print('Register done!')\r\n",
    "        self._ModelRegistry_[key] = model\r\n",
    "        return model\r\n",
    "\r\n",
    "    def get_model(self, key):\r\n",
    "        return self._ModelRegistry_[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38JPGe5YyAmN"
   },
   "source": [
    "### 1. Register all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_AI-tMfXPIEu"
   },
   "outputs": [],
   "source": [
    "manager = ModelManager()\r\n",
    "\r\n",
    "manager.register_model('unet_mobilenetv2', unet_model_1(3)),\r\n",
    "manager.register_model('unet_mobilenetv3', unet_model_2(3)),\r\n",
    "manager.register_model('unet_diy', unet_model_3(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WI6JVfrtyeZv"
   },
   "source": [
    "### 2. Brief information of Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1g_0cumpPKJ"
   },
   "outputs": [],
   "source": [
    "#@title Select a model to try { run: \"auto\", vertical-output: true, display-mode: \"form\" }\r\n",
    "model_name = \"unet_mobilenetv2\" #@param [\"unet_mobilenetv2\", \"unet_mobilenetv3\", \"unet_diy\"] {allow-input: true}\r\n",
    "print(model_name + \" is choose!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQ3F5MqP_YEI"
   },
   "outputs": [],
   "source": [
    "model = manager.get_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MuMzPbzAi82W"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uK7Z_iWljOKk"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhSEPdcdyH5u"
   },
   "source": [
    "### 3. Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RJ4JehvcvQ-"
   },
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3\r\n",
    "EPOCHS = 50\r\n",
    "VAL_SUBSPLITS = 5\r\n",
    "VALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS\r\n",
    "\r\n",
    "optimizer = tf.keras.optimizers.Adam(\r\n",
    "    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\r\n",
    "        initial_learning_rate = 0.03,\r\n",
    "        decay_steps=10000,\r\n",
    "        decay_rate=0.95,\r\n",
    "        staircase=True\r\n",
    "    )\r\n",
    ")\r\n",
    "\r\n",
    "loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
    "\r\n",
    "metrics = ['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIwl5Xi5i6Bi"
   },
   "outputs": [],
   "source": [
    "model.compile(\r\n",
    "    optimizer = optimizer,\r\n",
    "    loss = loss,\r\n",
    "    metrics = metrics\r\n",
    ")\r\n",
    "\r\n",
    "history = model.fit(\r\n",
    "    train_dataset, epochs=EPOCHS,\r\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\r\n",
    "    validation_steps=VALIDATION_STEPS,\r\n",
    "    validation_data=test_dataset,\r\n",
    "    callbacks=[\r\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5),\r\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\r\n",
    "    ]\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMok5Ux-dlXH"
   },
   "source": [
    "### 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Y0eiBWDePWZ"
   },
   "outputs": [],
   "source": [
    "loss = history.history['loss']\r\n",
    "val_loss = history.history['val_loss']\r\n",
    "\r\n",
    "epochs = range(len(loss))\r\n",
    "\r\n",
    "plt.figure()\r\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\r\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\r\n",
    "plt.title('Training and Validation Loss')\r\n",
    "plt.xlabel('Epoch')\r\n",
    "plt.ylabel('Loss Value')\r\n",
    "plt.ylim([0, max(loss)])\r\n",
    "plt.legend()\r\n",
    "plt.show()\r\n",
    "\r\n",
    "acc = history.history['accuracy']\r\n",
    "val_acc = history.history['val_accuracy']\r\n",
    "\r\n",
    "plt.figure()\r\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\r\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\r\n",
    "plt.title('Training and Validation Accuracy')\r\n",
    "plt.xlabel('Epoch')\r\n",
    "plt.ylabel('Accuracy')\r\n",
    "plt.ylim([0, 1])\r\n",
    "plt.legend()\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxAaSrdVTH39"
   },
   "outputs": [],
   "source": [
    "model.evaluate(\r\n",
    "    test_dataset, \r\n",
    "    batch_size=STEPS_PER_EPOCH\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yc_3RgFNy15G"
   },
   "source": [
    "## V. Application in real example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdlOb92gdk42"
   },
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\r\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)\r\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\r\n",
    "    return pred_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7NxMg4sdqrr"
   },
   "outputs": [],
   "source": [
    "def show_predictions(model, dataset=None, num=1):\r\n",
    "   \r\n",
    "    if dataset:\r\n",
    "        for image, mask in dataset.take(num):\r\n",
    "            pred_mask = create_mask(model.predict(image))\r\n",
    "            result = remove_background(\r\n",
    "                image[0],\r\n",
    "                tf.cast(pred_mask, tf.float32)\r\n",
    "            )\r\n",
    "            display([image[0], mask[0], pred_mask, result])\r\n",
    "    else:\r\n",
    "        pred_mask =  create_mask(model.predict(sample_image[tf.newaxis, ...]))\r\n",
    "        # result = remove_background(sample_image, pred_mask)\r\n",
    "        display([sample_image, sample_mask, pred_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdzR7oqdzAdg"
   },
   "source": [
    "### 1. From test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmSSRviDzE7I"
   },
   "outputs": [],
   "source": [
    "show_predictions(model, test_dataset, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBmB73VJzKAS"
   },
   "source": [
    "### 2. Let try your example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-ASHAWAzOmR"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "import cv2 as cv\r\n",
    "from google.colab import files\r\n",
    "uploaded = files.upload()\r\n",
    "\r\n",
    "for name, data in uploaded.items():\r\n",
    "  with open(name, 'wb') as f:\r\n",
    "    f.write(data)\r\n",
    "    print ('saved file', name)\r\n",
    "\r\n",
    "img = cv.imread(name)\r\n",
    "img_cvt=cv.cvtColor(img, cv.COLOR_BGR2RGB)\r\n",
    "plt.imshow(img_cvt)\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUthBaCh40ys"
   },
   "outputs": [],
   "source": [
    "resize_img = cv.resize(img, IMAGE_SIZE)\r\n",
    "\r\n",
    "reshape_img = np.reshape(resize_img , (1, resize_img.shape[0], resize_img.shape[1], resize_img.shape[2]))\r\n",
    "\r\n",
    "pred_mask = create_mask(model.predict(reshape_img))\r\n",
    "\r\n",
    "result = remove_background(\r\n",
    "    resize_img,\r\n",
    "    tf.cast(pred_mask, tf.float32)\r\n",
    ")\r\n",
    "\r\n",
    "display([cv.cvtColor(resize_img, cv.COLOR_BGR2RGB), pred_mask, result], [True, False, True, True])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Unet_Image_Segment.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
